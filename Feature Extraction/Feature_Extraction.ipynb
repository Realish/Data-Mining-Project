{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install and import the required packages**"
      ],
      "metadata": {
        "id": "qemF2pw7kn2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install keras-core"
      ],
      "metadata": {
        "id": "Uv9HPnsgkyRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import keras_core\n",
        "import torch\n",
        "from IPython.display import Markdown, HTML\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyspark\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "xxYapAnYlBui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "D-HnCTJkxMPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "data_dir = \"/content/gdrive/My Drive/Colab Notebooks/Data/genres_original\"\n",
        "genre_dirs = [x[0] for x in os.walk(data_dir)]\n",
        "\n",
        "filepath_dict = {}\n",
        "for genre_dir in genre_dirs[1:]:\n",
        "    filepath_list = []\n",
        "    for fn in os.listdir(genre_dir):\n",
        "      filepath_list.append(os.path.join(genre_dir, fn))\n",
        "\n",
        "    filepath_dict[genre_dir.split('/')[-1]] = filepath_list\n",
        "\n",
        "filepath_dict.keys()"
      ],
      "metadata": {
        "id": "uIDRGsI7v6eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display sample audio and waveform of each genre**"
      ],
      "metadata": {
        "id": "ZJMMgpATxTMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for key in filepath_dict.keys():\n",
        "  #extract the audio sample\n",
        "  y, sr = librosa.load(filepath_dict[key][0])\n",
        "  print(f\"{key} sampling rate is: {sr}\")\n",
        "  display(HTML(f\"<h3 style='text-align: left'>Audio for {key}</h3>\\n\"))\n",
        "  audio_widget = ipd.Audio(data=y, rate=sr)\n",
        "  display(audio_widget)\n",
        "\n",
        "  #extract waveform plot\n",
        "  fig_waveform, ax_waveform = plt.subplots(figsize=(11.23, 3))\n",
        "  librosa.display.waveshow(y, sr=sr, ax=ax_waveform)\n",
        "  ax_waveform.set(title='{} sample waveform'.format(key), xlabel='Time (s)',\n",
        "                  ylabel='Amplitude', xlim=[0, len(y)/sr])\n",
        "  ax_waveform.yaxis.set_major_formatter(ticker.FormatStrFormatter('%.2f'))\n",
        "  plt.show()\n",
        "\n",
        "  #extract STFT spectrogram and plot in log-scale\n",
        "  fig_spectrogram, ax_spectrogram = plt.subplots(figsize=(14, 5))  # Slightly wider figure for the spectrogram\n",
        "  FRAME_SIZE = 1024\n",
        "  HOP_SIZE = 5201\n",
        "  D = librosa.stft(y, n_fft=FRAME_SIZE, hop_length=HOP_SIZE)\n",
        "  S, phase = librosa.magphase(D)\n",
        "  S_db = librosa.amplitude_to_db(S, ref=np.max)\n",
        "  img = librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='log',\n",
        "                                 hop_length=HOP_SIZE, ax=ax_spectrogram)\n",
        "  ax_spectrogram.set(title='{} STFT Spectrogram'.format(key),\n",
        "                     xlabel='Time (s)', ylabel='Frequency (Hz)')\n",
        "  fig_spectrogram.colorbar(img, ax=ax_spectrogram, format='%+1.0f dB')  # Add colorbar to the spectrogram figure\n",
        "  plt.show()\n",
        "  print(200*\"-\")"
      ],
      "metadata": {
        "id": "jNdTy5471DRO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the serial attention model (SAM)"
      ],
      "metadata": {
        "id": "OEdejS7aYp3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class BiGRUEncoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
        "        super(BiGRUEncoder, self).__init__()\n",
        "        self.bigru = nn.GRU(input_dim, hidden_dim, num_layers,\n",
        "                            batch_first=True, dropout=dropout, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs, _ = self.bigru(x)  # outputs shape: (batch_size, sequence_length, 2 * hidden_dim)\n",
        "        return outputs\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_vector = nn.Parameter(torch.randn(hidden_dim * 2, 1))\n",
        "\n",
        "    def forward(self, encoder_outputs):\n",
        "        # Linear transformation using a trainable attention vector\n",
        "        attention_scores = torch.matmul(encoder_outputs, self.attention_vector)  # shape: (batch_size, sequence_length, 1)\n",
        "        attention_scores = attention_scores.squeeze(-1)  # shape: (batch_size, sequence_length)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)  # shape: (batch_size, sequence_length)\n",
        "        return attention_weights\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, hidden_dim, num_classes):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, weighted_representation):\n",
        "        logits = self.fc(weighted_representation)\n",
        "        return logits\n",
        "\n",
        "class SAMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, dropout, num_classes):\n",
        "        super(SAMModel, self).__init__()\n",
        "        self.encoder = BiGRUEncoder(input_dim, hidden_dim, num_layers, dropout)\n",
        "        self.attention = Attention(hidden_dim)\n",
        "        self.classifier = Classifier(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoder_outputs = self.encoder(x)\n",
        "        attention_weights = self.attention(encoder_outputs)\n",
        "        weighted_representation = torch.sum(encoder_outputs * attention_weights.unsqueeze(-1), dim=1)\n",
        "        logits = self.classifier(weighted_representation)\n",
        "        return logits\n",
        "\n",
        "# Example of model initialization and forward pass\n",
        "input_dim = 128  # Example feature size for each timestep\n",
        "hidden_dim = 256  # Size of GRU hidden states\n",
        "num_layers = 2  # Number of stacked GRU layers\n",
        "dropout = 0.5  # Dropout between RNN layers\n",
        "num_classes = 10  # Example number of output classes (e.g., genres)\n",
        "\n",
        "model = SAMModel(input_dim, hidden_dim, num_layers, dropout, num_classes)\n",
        "\n",
        "# Example spectrogram input (batch_size, sequence_length, input_dim)\n",
        "spectrogram_input = torch.randn(32, 100, input_dim)\n",
        "\n",
        "# Forward pass\n",
        "logits = model(spectrogram_input)\n"
      ],
      "metadata": {
        "id": "aszR4rm_7jy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusting the SAM model\n",
        "class SAM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bidirectional=True):\n",
        "        super(SAM, self).__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=2,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidirectional\n",
        "        )\n",
        "        self.attention_weights = nn.Parameter(torch.randn(2 * hidden_size if bidirectional else hidden_size, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        gru_out, _ = self.gru(x)  # GRU output shape: (batch, seq_length, num_directions * hidden_size)\n",
        "        attention_scores = torch.matmul(gru_out, self.attention_weights).squeeze(2)  # Shape: (batch, seq_length)\n",
        "        attention_probs = F.softmax(attention_scores, dim=1).unsqueeze(2)  # Shape: (batch, seq_length, 1)\n",
        "        weighted_gru_out = gru_out * attention_probs  # Shape: (batch, seq_length, num_directions * hidden_size)\n",
        "        attended_representation = weighted_gru_out.sum(dim=1)  # Shape: (batch, num_directions * hidden_size)\n",
        "        return attended_representation\n",
        "\n",
        "# Assuming input size is 513 based on the input shape of the spectrogram (513 features for each time step)\n",
        "sam_model = SAM(input_size=513, hidden_size=256)\n"
      ],
      "metadata": {
        "id": "wQ8himylY0Sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PCNNA(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PCNNA, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
        "        self.conv5 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(64)\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        # This will hold the linear layer, once we know the input dimensions\n",
        "        self.fc = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.pool2(x)\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.pool3(x)\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.pool4(x)\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = self.pool5(x)\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # Dynamically create the fully connected layer if it's not already defined\n",
        "        if self.fc is None:\n",
        "            # Assuming the dense layer is meant to output a vector of size equal to the flattened features\n",
        "            T = x.shape[1]\n",
        "            self.fc = nn.Linear(T, T).to(x.device)\n",
        "\n",
        "        attention_scores = self.fc(x)\n",
        "        return attention_scores\n",
        "\n",
        "# Now we can instantiate the PAM model without specifying T\n",
        "pam_model = PCNNA()\n",
        "\n",
        "# To deal with different input sizes, simply call the forward method\n",
        "# The first time you call it, it will automatically adjust the fully connected layer\n",
        "dummy_input = torch.randn(1, 1, 513, 128)  # Replace with actual dimensions\n",
        "pam_output = pam_model(dummy_input)\n"
      ],
      "metadata": {
        "id": "F_lHHCkDc6OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UzdKCElFdCvz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}