{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, recall_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/kevin_smith/Desktop/FSU_Relevant_Stuff/fall_2023/CAP5771/project/gtzan/Data/features_30_sec.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Important Features:\n",
      "            Feature  Coefficient\n",
      "24       mfcc4_mean     0.249012\n",
      "27        mfcc5_var     0.235593\n",
      "28       mfcc6_mean     0.207043\n",
      "52      mfcc18_mean    -0.197398\n",
      "50      mfcc17_mean    -0.186486\n",
      "22       mfcc3_mean     0.168322\n",
      "2   chroma_stft_var     0.167185\n",
      "29        mfcc6_var     0.164697\n",
      "18       mfcc1_mean     0.145843\n",
      "44      mfcc14_mean     0.133247\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features (X) and labels (y)\n",
    "X = df.drop(['filename', 'label'], axis=1)\n",
    "y = df['label']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Standardize the features (optional but often recommended for SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train an SVM model\n",
    "svm_model = SVC(kernel='linear', C=1.0)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extract coefficients from the SVM model\n",
    "coefficients = svm_model.coef_[0]\n",
    "\n",
    "# Create a DataFrame to store feature names and their corresponding coefficients\n",
    "feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': coefficients})\n",
    "\n",
    "# Sort the DataFrame by the absolute values of coefficients in descending order\n",
    "feature_importance_df = feature_importance_df.reindex(feature_importance_df['Coefficient'].abs().sort_values(ascending=False).index)\n",
    "\n",
    "# Print the top N most important features and their coefficients\n",
    "top_n = 10  # Change this value to print a different number of top features\n",
    "print(f\"Top {top_n} Most Important Features:\")\n",
    "print(feature_importance_df.head(top_n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Recall: 0.695\n",
      "F1 Score: 0.6927321363374466\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights to handle class imbalance\n",
    "class_weights = compute_class_weight('balanced', classes=pd.unique(y_train), y=y_train)\n",
    "class_weight_dict = dict(zip(pd.unique(y_train), class_weights))\n",
    "\n",
    "# Initialize SVM model\n",
    "svm_model = SVC(class_weight=class_weight_dict)\n",
    "\n",
    "# Define scoring metrics (recall and F1 score)\n",
    "scoring = {'recall': make_scorer(recall_score, average='weighted'),\n",
    "           'f1_score': make_scorer(f1_score, average='weighted')}\n",
    "\n",
    "# Hyperparameter tuning using grid search\n",
    "param_grid = {'C': [0.1, 1, 10, 100],\n",
    "              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              'gamma': ['scale', 'auto', 0.1, 1, 10]}  # Adjust gamma for RBF kernel\n",
    "grid_search = GridSearchCV(svm_model, param_grid, scoring=scoring, cv=5, n_jobs=-1, refit='f1_score')\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Extract the best model\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_svm_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model performance\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Display the results\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
